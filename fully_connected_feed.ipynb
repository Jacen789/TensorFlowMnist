{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import gzip\n",
    "import urllib\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read32(bytestream):\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(f):\n",
    "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "    print('Extracting', f.name)\n",
    "    with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Invalid magic number %d in MNIST image file: %s' % (magic, f.name))\n",
    "        num_images = _read32(bytestream)\n",
    "        rows = _read32(bytestream)\n",
    "        cols = _read32(bytestream)\n",
    "        buf = bytestream.read(rows * cols * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8)\n",
    "        data = data.reshape(num_images, rows, cols, 1)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(f, one_hot=False, num_classes=10):\n",
    "    \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n",
    "    print('Extracting', f.name)\n",
    "    with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Invalid magic number %d in MNIST label file: %s' % (magic, f.name))\n",
    "        num_items = _read32(bytestream)\n",
    "        buf = bytestream.read(num_items)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8)\n",
    "        if one_hot:\n",
    "            return dense_to_one_hot(labels, num_classes)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    \"\"\"Container class for a dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, images, labels, fake_data=False, one_hot=False, dtype=tf.float32, reshape=True, seed=None):\n",
    "        \"\"\"Construct a DataSet.\"\"\"\n",
    "        seed1, seed2 = tf.get_seed(seed)\n",
    "        # If op level seed is not set, use whatever graph level seed is returned\n",
    "        np.random.seed(seed1 if seed is None else seed2)\n",
    "        dtype = tf.as_dtype(dtype).base_dtype\n",
    "        if dtype not in (tf.uint8, tf.float32):\n",
    "            raise TypeError('Invalid image dtype %r, expected uint8 or float32' % dtype)\n",
    "        if fake_data:\n",
    "            self._num_examples = 10000\n",
    "            self.one_hot = one_hot\n",
    "        else:\n",
    "            assert images.shape[0] == labels.shape[0], ('images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n",
    "            self._num_examples = images.shape[0]\n",
    "\n",
    "            # Convert shape from [num examples, rows, columns, depth]\n",
    "            # to [num examples, rows*columns] (assuming depth == 1)\n",
    "            if reshape:\n",
    "                assert images.shape[3] == 1\n",
    "                images = images.reshape(images.shape[0], images.shape[1] * images.shape[2])\n",
    "            if dtype == tf.float32:\n",
    "                # Convert from [0, 255] -> [0.0, 1.0].\n",
    "                images = images.astype(np.float32)\n",
    "                images = np.multiply(images, 1.0 / 255.0)\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size, fake_data=False, shuffle=True):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        if fake_data:\n",
    "            fake_image = [1] * 784\n",
    "            if self.one_hot:\n",
    "                fake_label = [1] + [0] * 9\n",
    "            else:\n",
    "                fake_label = 0\n",
    "            return [fake_image for _ in range(batch_size)], [fake_label for _ in range(batch_size)]\n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for the first epoch\n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._images = self.images[perm0]\n",
    "            self._labels = self.labels[perm0]\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            images_rest_part = self._images[start:self._num_examples]\n",
    "            labels_rest_part = self._labels[start:self._num_examples]\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                perm = np.arange(self._num_examples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._images = self.images[perm]\n",
    "                self._labels = self.labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            images_new_part = self._images[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return np.concatenate((images_rest_part, images_new_part), axis=0), np.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._images[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlretrieve_with_retry(url, filename=None):\n",
    "    return urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(filename, work_directory, source_url):\n",
    "    \"\"\"Download the data from source url, unless it's already here.\"\"\"\n",
    "    if not tf.gfile.Exists(work_directory):\n",
    "        tf.gfile.MakeDirs(work_directory)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    if not tf.gfile.Exists(filepath):\n",
    "        temp_file_name, _ = urlretrieve_with_retry(source_url)\n",
    "        tf.gfile.Copy(temp_file_name, filepath)\n",
    "        with tf.gfile.GFile(filepath) as f:\n",
    "            size = f.size()\n",
    "        print('Successfully downloaded', filename, size, 'bytes.')\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_sets(train_dir, fake_data=False, one_hot=False, dtype=tf.float32, reshape=True, validation_size=5000, seed=None, source_url=DEFAULT_SOURCE_URL):\n",
    "    if fake_data:\n",
    "        def fake():\n",
    "            return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype, seed=seed)\n",
    "\n",
    "        train = fake()\n",
    "        validation = fake()\n",
    "        test = fake()\n",
    "        return Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "    if not source_url:  # empty string check\n",
    "        source_url = DEFAULT_SOURCE_URL\n",
    "\n",
    "    TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "    TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "    TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "    TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "    local_file = maybe_download(TRAIN_IMAGES, train_dir, source_url + TRAIN_IMAGES)\n",
    "    with tf.gfile.Open(local_file, 'rb') as f:\n",
    "        train_images = extract_images(f)\n",
    "\n",
    "    local_file = maybe_download(TRAIN_LABELS, train_dir, source_url + TRAIN_LABELS)\n",
    "    with tf.gfile.Open(local_file, 'rb') as f:\n",
    "        train_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "    local_file = maybe_download(TEST_IMAGES, train_dir, source_url + TEST_IMAGES)\n",
    "    with tf.gfile.Open(local_file, 'rb') as f:\n",
    "        test_images = extract_images(f)\n",
    "\n",
    "    local_file = maybe_download(TEST_LABELS, train_dir, source_url + TEST_LABELS)\n",
    "    with tf.gfile.Open(local_file, 'rb') as f:\n",
    "        test_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "    if not 0 <= validation_size <= len(train_images):\n",
    "        raise ValueError('Validation size should be between 0 and {}. Received: {}.'.format(len(train_images), validation_size))\n",
    "\n",
    "    validation_images = train_images[:validation_size]\n",
    "    validation_labels = train_labels[:validation_size]\n",
    "    train_images = train_images[validation_size:]\n",
    "    train_labels = train_labels[validation_size:]\n",
    "\n",
    "    options = dict(dtype=dtype, reshape=reshape, seed=seed)\n",
    "\n",
    "    train = DataSet(train_images, train_labels, **options)\n",
    "    validation = DataSet(validation_images, validation_labels, **options)\n",
    "    test = DataSet(test_images, test_labels, **options)\n",
    "\n",
    "    return Datasets(train=train, validation=validation, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "max_steps = 50000\n",
    "hidden1_units = 128\n",
    "hidden2_units = 32\n",
    "batch_size = 100\n",
    "input_data_dir = 'input_data'\n",
    "log_dir = 'logs/fully_connected_feed'\n",
    "fake_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.DeleteRecursively(log_dir)\n",
    "tf.gfile.MakeDirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting input_data/train-images-idx3-ubyte.gz\n",
      "Extracting input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_sets = read_data_sets(input_data_dir, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS), name='images')\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size), name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden 1\n",
    "weights = tf.Variable(\n",
    "    tf.truncated_normal([IMAGE_PIXELS, hidden1_units], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "    name='weights')\n",
    "biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights) + biases)\n",
    "# Hidden 2\n",
    "weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden1_units, hidden2_units], stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "    name='weights')\n",
    "biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "# Linear\n",
    "weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden2_units, NUM_CLASSES], stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "    name='weights')\n",
    "biases = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "logits = tf.matmul(hidden2, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.to_int64(labels_placeholder)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(logits, 1)\n",
    "correct = tf.equal(prediction, labels)\n",
    "eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    \"\"\"Fills the feed_dict for training the given step.\"\"\"\n",
    "    images_feed, labels_feed = data_set.next_batch(batch_size, fake_data)\n",
    "    feed_dict = {images_pl: images_feed, labels_pl: labels_feed}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of input_data.\"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set, images_placeholder, labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    precision = float(true_count) / num_examples\n",
    "    print('Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.32 (0.141 sec)\n",
      "Step 100: loss = 2.14 (0.002 sec)\n",
      "Step 200: loss = 1.90 (0.002 sec)\n",
      "Step 300: loss = 1.58 (0.002 sec)\n",
      "Step 400: loss = 1.20 (0.002 sec)\n",
      "Step 500: loss = 0.93 (0.002 sec)\n",
      "Step 600: loss = 0.81 (0.004 sec)\n",
      "Step 700: loss = 0.77 (0.002 sec)\n",
      "Step 800: loss = 0.61 (0.002 sec)\n",
      "Step 900: loss = 0.61 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 47047  Precision @ 1: 0.8554\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4318  Precision @ 1: 0.8636\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 8656  Precision @ 1: 0.8656\n",
      "Step 1000: loss = 0.53 (0.009 sec)\n",
      "Step 1100: loss = 0.44 (0.100 sec)\n",
      "Step 1200: loss = 0.61 (0.002 sec)\n",
      "Step 1300: loss = 0.35 (0.005 sec)\n",
      "Step 1400: loss = 0.43 (0.003 sec)\n",
      "Step 1500: loss = 0.35 (0.006 sec)\n",
      "Step 1600: loss = 0.44 (0.002 sec)\n",
      "Step 1700: loss = 0.46 (0.002 sec)\n",
      "Step 1800: loss = 0.40 (0.002 sec)\n",
      "Step 1900: loss = 0.22 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 49412  Precision @ 1: 0.8984\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4512  Precision @ 1: 0.9024\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9027  Precision @ 1: 0.9027\n",
      "Step 2000: loss = 0.44 (0.014 sec)\n",
      "Step 2100: loss = 0.39 (0.003 sec)\n",
      "Step 2200: loss = 0.25 (0.101 sec)\n",
      "Step 2300: loss = 0.29 (0.002 sec)\n",
      "Step 2400: loss = 0.37 (0.002 sec)\n",
      "Step 2500: loss = 0.29 (0.002 sec)\n",
      "Step 2600: loss = 0.38 (0.002 sec)\n",
      "Step 2700: loss = 0.29 (0.002 sec)\n",
      "Step 2800: loss = 0.28 (0.002 sec)\n",
      "Step 2900: loss = 0.39 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 50035  Precision @ 1: 0.9097\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4587  Precision @ 1: 0.9174\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9150  Precision @ 1: 0.9150\n",
      "Step 3000: loss = 0.27 (0.012 sec)\n",
      "Step 3100: loss = 0.38 (0.002 sec)\n",
      "Step 3200: loss = 0.37 (0.002 sec)\n",
      "Step 3300: loss = 0.26 (0.101 sec)\n",
      "Step 3400: loss = 0.24 (0.002 sec)\n",
      "Step 3500: loss = 0.34 (0.002 sec)\n",
      "Step 3600: loss = 0.31 (0.002 sec)\n",
      "Step 3700: loss = 0.24 (0.002 sec)\n",
      "Step 3800: loss = 0.21 (0.002 sec)\n",
      "Step 3900: loss = 0.26 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 50534  Precision @ 1: 0.9188\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4629  Precision @ 1: 0.9258\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9235  Precision @ 1: 0.9235\n",
      "Step 4000: loss = 0.37 (0.011 sec)\n",
      "Step 4100: loss = 0.38 (0.002 sec)\n",
      "Step 4200: loss = 0.25 (0.002 sec)\n",
      "Step 4300: loss = 0.17 (0.002 sec)\n",
      "Step 4400: loss = 0.30 (0.106 sec)\n",
      "Step 4500: loss = 0.15 (0.002 sec)\n",
      "Step 4600: loss = 0.55 (0.002 sec)\n",
      "Step 4700: loss = 0.39 (0.002 sec)\n",
      "Step 4800: loss = 0.38 (0.002 sec)\n",
      "Step 4900: loss = 0.33 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 50885  Precision @ 1: 0.9252\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4656  Precision @ 1: 0.9312\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9284  Precision @ 1: 0.9284\n",
      "Step 5000: loss = 0.28 (0.010 sec)\n",
      "Step 5100: loss = 0.23 (0.002 sec)\n",
      "Step 5200: loss = 0.31 (0.002 sec)\n",
      "Step 5300: loss = 0.22 (0.002 sec)\n",
      "Step 5400: loss = 0.35 (0.002 sec)\n",
      "Step 5500: loss = 0.21 (0.110 sec)\n",
      "Step 5600: loss = 0.14 (0.002 sec)\n",
      "Step 5700: loss = 0.26 (0.002 sec)\n",
      "Step 5800: loss = 0.20 (0.002 sec)\n",
      "Step 5900: loss = 0.18 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 51210  Precision @ 1: 0.9311\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4686  Precision @ 1: 0.9372\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9322  Precision @ 1: 0.9322\n",
      "Step 6000: loss = 0.41 (0.014 sec)\n",
      "Step 6100: loss = 0.16 (0.002 sec)\n",
      "Step 6200: loss = 0.13 (0.002 sec)\n",
      "Step 6300: loss = 0.19 (0.002 sec)\n",
      "Step 6400: loss = 0.11 (0.002 sec)\n",
      "Step 6500: loss = 0.19 (0.003 sec)\n",
      "Step 6600: loss = 0.26 (0.101 sec)\n",
      "Step 6700: loss = 0.17 (0.002 sec)\n",
      "Step 6800: loss = 0.24 (0.002 sec)\n",
      "Step 6900: loss = 0.21 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 51581  Precision @ 1: 0.9378\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4705  Precision @ 1: 0.9410\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9381  Precision @ 1: 0.9381\n",
      "Step 7000: loss = 0.16 (0.010 sec)\n",
      "Step 7100: loss = 0.28 (0.003 sec)\n",
      "Step 7200: loss = 0.12 (0.002 sec)\n",
      "Step 7300: loss = 0.31 (0.003 sec)\n",
      "Step 7400: loss = 0.19 (0.002 sec)\n",
      "Step 7500: loss = 0.28 (0.002 sec)\n",
      "Step 7600: loss = 0.27 (0.002 sec)\n",
      "Step 7700: loss = 0.13 (0.105 sec)\n",
      "Step 7800: loss = 0.10 (0.004 sec)\n",
      "Step 7900: loss = 0.29 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 51739  Precision @ 1: 0.9407\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4732  Precision @ 1: 0.9464\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9405  Precision @ 1: 0.9405\n",
      "Step 8000: loss = 0.22 (0.013 sec)\n",
      "Step 8100: loss = 0.14 (0.002 sec)\n",
      "Step 8200: loss = 0.30 (0.010 sec)\n",
      "Step 8300: loss = 0.13 (0.004 sec)\n",
      "Step 8400: loss = 0.16 (0.002 sec)\n",
      "Step 8500: loss = 0.13 (0.002 sec)\n",
      "Step 8600: loss = 0.19 (0.002 sec)\n",
      "Step 8700: loss = 0.18 (0.002 sec)\n",
      "Step 8800: loss = 0.17 (0.108 sec)\n",
      "Step 8900: loss = 0.20 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 51909  Precision @ 1: 0.9438\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4738  Precision @ 1: 0.9476\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9432  Precision @ 1: 0.9432\n",
      "Step 9000: loss = 0.15 (0.011 sec)\n",
      "Step 9100: loss = 0.17 (0.002 sec)\n",
      "Step 9200: loss = 0.14 (0.002 sec)\n",
      "Step 9300: loss = 0.25 (0.002 sec)\n",
      "Step 9400: loss = 0.13 (0.002 sec)\n",
      "Step 9500: loss = 0.08 (0.002 sec)\n",
      "Step 9600: loss = 0.13 (0.002 sec)\n",
      "Step 9700: loss = 0.14 (0.002 sec)\n",
      "Step 9800: loss = 0.19 (0.002 sec)\n",
      "Step 9900: loss = 0.15 (0.108 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52195  Precision @ 1: 0.9490\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4754  Precision @ 1: 0.9508\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9461  Precision @ 1: 0.9461\n",
      "Step 10000: loss = 0.24 (0.009 sec)\n",
      "Step 10100: loss = 0.28 (0.002 sec)\n",
      "Step 10200: loss = 0.16 (0.002 sec)\n",
      "Step 10300: loss = 0.14 (0.002 sec)\n",
      "Step 10400: loss = 0.12 (0.002 sec)\n",
      "Step 10500: loss = 0.24 (0.002 sec)\n",
      "Step 10600: loss = 0.19 (0.002 sec)\n",
      "Step 10700: loss = 0.28 (0.002 sec)\n",
      "Step 10800: loss = 0.15 (0.002 sec)\n",
      "Step 10900: loss = 0.13 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52295  Precision @ 1: 0.9508\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4767  Precision @ 1: 0.9534\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9497  Precision @ 1: 0.9497\n",
      "Step 11000: loss = 0.12 (0.111 sec)\n",
      "Step 11100: loss = 0.15 (0.002 sec)\n",
      "Step 11200: loss = 0.17 (0.003 sec)\n",
      "Step 11300: loss = 0.19 (0.002 sec)\n",
      "Step 11400: loss = 0.17 (0.002 sec)\n",
      "Step 11500: loss = 0.14 (0.002 sec)\n",
      "Step 11600: loss = 0.14 (0.002 sec)\n",
      "Step 11700: loss = 0.24 (0.002 sec)\n",
      "Step 11800: loss = 0.15 (0.002 sec)\n",
      "Step 11900: loss = 0.24 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52410  Precision @ 1: 0.9529\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4773  Precision @ 1: 0.9546\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9507  Precision @ 1: 0.9507\n",
      "Step 12000: loss = 0.07 (0.010 sec)\n",
      "Step 12100: loss = 0.14 (0.100 sec)\n",
      "Step 12200: loss = 0.12 (0.002 sec)\n",
      "Step 12300: loss = 0.16 (0.002 sec)\n",
      "Step 12400: loss = 0.09 (0.002 sec)\n",
      "Step 12500: loss = 0.12 (0.002 sec)\n",
      "Step 12600: loss = 0.15 (0.002 sec)\n",
      "Step 12700: loss = 0.17 (0.002 sec)\n",
      "Step 12800: loss = 0.38 (0.002 sec)\n",
      "Step 12900: loss = 0.16 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52564  Precision @ 1: 0.9557\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4796  Precision @ 1: 0.9592\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9528  Precision @ 1: 0.9528\n",
      "Step 13000: loss = 0.10 (0.010 sec)\n",
      "Step 13100: loss = 0.07 (0.002 sec)\n",
      "Step 13200: loss = 0.08 (0.143 sec)\n",
      "Step 13300: loss = 0.20 (0.002 sec)\n",
      "Step 13400: loss = 0.14 (0.002 sec)\n",
      "Step 13500: loss = 0.17 (0.002 sec)\n",
      "Step 13600: loss = 0.11 (0.002 sec)\n",
      "Step 13700: loss = 0.22 (0.002 sec)\n",
      "Step 13800: loss = 0.10 (0.002 sec)\n",
      "Step 13900: loss = 0.15 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52769  Precision @ 1: 0.9594\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4798  Precision @ 1: 0.9596\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9543  Precision @ 1: 0.9543\n",
      "Step 14000: loss = 0.12 (0.011 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14100: loss = 0.08 (0.002 sec)\n",
      "Step 14200: loss = 0.09 (0.002 sec)\n",
      "Step 14300: loss = 0.07 (0.101 sec)\n",
      "Step 14400: loss = 0.16 (0.002 sec)\n",
      "Step 14500: loss = 0.14 (0.002 sec)\n",
      "Step 14600: loss = 0.10 (0.002 sec)\n",
      "Step 14700: loss = 0.26 (0.002 sec)\n",
      "Step 14800: loss = 0.21 (0.002 sec)\n",
      "Step 14900: loss = 0.18 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52886  Precision @ 1: 0.9616\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4809  Precision @ 1: 0.9618\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9561  Precision @ 1: 0.9561\n",
      "Step 15000: loss = 0.10 (0.010 sec)\n",
      "Step 15100: loss = 0.16 (0.002 sec)\n",
      "Step 15200: loss = 0.21 (0.002 sec)\n",
      "Step 15300: loss = 0.10 (0.002 sec)\n",
      "Step 15400: loss = 0.18 (0.103 sec)\n",
      "Step 15500: loss = 0.25 (0.002 sec)\n",
      "Step 15600: loss = 0.18 (0.007 sec)\n",
      "Step 15700: loss = 0.08 (0.002 sec)\n",
      "Step 15800: loss = 0.07 (0.002 sec)\n",
      "Step 15900: loss = 0.08 (0.003 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 52923  Precision @ 1: 0.9622\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4814  Precision @ 1: 0.9628\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9583  Precision @ 1: 0.9583\n",
      "Step 16000: loss = 0.23 (0.014 sec)\n",
      "Step 16100: loss = 0.12 (0.002 sec)\n",
      "Step 16200: loss = 0.11 (0.002 sec)\n",
      "Step 16300: loss = 0.18 (0.002 sec)\n",
      "Step 16400: loss = 0.12 (0.002 sec)\n",
      "Step 16500: loss = 0.16 (0.100 sec)\n",
      "Step 16600: loss = 0.13 (0.002 sec)\n",
      "Step 16700: loss = 0.11 (0.002 sec)\n",
      "Step 16800: loss = 0.12 (0.002 sec)\n",
      "Step 16900: loss = 0.10 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53044  Precision @ 1: 0.9644\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4826  Precision @ 1: 0.9652\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9602  Precision @ 1: 0.9602\n",
      "Step 17000: loss = 0.11 (0.015 sec)\n",
      "Step 17100: loss = 0.09 (0.002 sec)\n",
      "Step 17200: loss = 0.10 (0.002 sec)\n",
      "Step 17300: loss = 0.12 (0.002 sec)\n",
      "Step 17400: loss = 0.08 (0.002 sec)\n",
      "Step 17500: loss = 0.23 (0.002 sec)\n",
      "Step 17600: loss = 0.12 (0.108 sec)\n",
      "Step 17700: loss = 0.17 (0.002 sec)\n",
      "Step 17800: loss = 0.18 (0.002 sec)\n",
      "Step 17900: loss = 0.06 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53094  Precision @ 1: 0.9653\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4829  Precision @ 1: 0.9658\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9613  Precision @ 1: 0.9613\n",
      "Step 18000: loss = 0.11 (0.011 sec)\n",
      "Step 18100: loss = 0.06 (0.002 sec)\n",
      "Step 18200: loss = 0.07 (0.002 sec)\n",
      "Step 18300: loss = 0.07 (0.002 sec)\n",
      "Step 18400: loss = 0.10 (0.002 sec)\n",
      "Step 18500: loss = 0.10 (0.002 sec)\n",
      "Step 18600: loss = 0.06 (0.002 sec)\n",
      "Step 18700: loss = 0.14 (0.107 sec)\n",
      "Step 18800: loss = 0.21 (0.002 sec)\n",
      "Step 18900: loss = 0.10 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53226  Precision @ 1: 0.9677\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4830  Precision @ 1: 0.9660\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9620  Precision @ 1: 0.9620\n",
      "Step 19000: loss = 0.10 (0.011 sec)\n",
      "Step 19100: loss = 0.09 (0.002 sec)\n",
      "Step 19200: loss = 0.07 (0.002 sec)\n",
      "Step 19300: loss = 0.15 (0.002 sec)\n",
      "Step 19400: loss = 0.11 (0.002 sec)\n",
      "Step 19500: loss = 0.14 (0.002 sec)\n",
      "Step 19600: loss = 0.10 (0.002 sec)\n",
      "Step 19700: loss = 0.11 (0.002 sec)\n",
      "Step 19800: loss = 0.09 (0.111 sec)\n",
      "Step 19900: loss = 0.05 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53260  Precision @ 1: 0.9684\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4833  Precision @ 1: 0.9666\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9630  Precision @ 1: 0.9630\n",
      "Step 20000: loss = 0.12 (0.010 sec)\n",
      "Step 20100: loss = 0.16 (0.002 sec)\n",
      "Step 20200: loss = 0.21 (0.002 sec)\n",
      "Step 20300: loss = 0.08 (0.002 sec)\n",
      "Step 20400: loss = 0.09 (0.002 sec)\n",
      "Step 20500: loss = 0.13 (0.002 sec)\n",
      "Step 20600: loss = 0.21 (0.002 sec)\n",
      "Step 20700: loss = 0.13 (0.002 sec)\n",
      "Step 20800: loss = 0.09 (0.002 sec)\n",
      "Step 20900: loss = 0.09 (0.105 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53317  Precision @ 1: 0.9694\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4840  Precision @ 1: 0.9680\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9643  Precision @ 1: 0.9643\n",
      "Step 21000: loss = 0.16 (0.011 sec)\n",
      "Step 21100: loss = 0.08 (0.002 sec)\n",
      "Step 21200: loss = 0.18 (0.002 sec)\n",
      "Step 21300: loss = 0.09 (0.002 sec)\n",
      "Step 21400: loss = 0.03 (0.005 sec)\n",
      "Step 21500: loss = 0.17 (0.002 sec)\n",
      "Step 21600: loss = 0.08 (0.002 sec)\n",
      "Step 21700: loss = 0.07 (0.002 sec)\n",
      "Step 21800: loss = 0.11 (0.002 sec)\n",
      "Step 21900: loss = 0.08 (0.003 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53459  Precision @ 1: 0.9720\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4844  Precision @ 1: 0.9688\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9649  Precision @ 1: 0.9649\n",
      "Step 22000: loss = 0.20 (0.110 sec)\n",
      "Step 22100: loss = 0.26 (0.002 sec)\n",
      "Step 22200: loss = 0.09 (0.003 sec)\n",
      "Step 22300: loss = 0.09 (0.003 sec)\n",
      "Step 22400: loss = 0.10 (0.002 sec)\n",
      "Step 22500: loss = 0.07 (0.002 sec)\n",
      "Step 22600: loss = 0.13 (0.002 sec)\n",
      "Step 22700: loss = 0.13 (0.002 sec)\n",
      "Step 22800: loss = 0.12 (0.003 sec)\n",
      "Step 22900: loss = 0.07 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53554  Precision @ 1: 0.9737\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4852  Precision @ 1: 0.9704\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9658  Precision @ 1: 0.9658\n",
      "Step 23000: loss = 0.11 (0.010 sec)\n",
      "Step 23100: loss = 0.06 (0.116 sec)\n",
      "Step 23200: loss = 0.05 (0.002 sec)\n",
      "Step 23300: loss = 0.15 (0.002 sec)\n",
      "Step 23400: loss = 0.08 (0.002 sec)\n",
      "Step 23500: loss = 0.09 (0.002 sec)\n",
      "Step 23600: loss = 0.29 (0.002 sec)\n",
      "Step 23700: loss = 0.10 (0.002 sec)\n",
      "Step 23800: loss = 0.09 (0.002 sec)\n",
      "Step 23900: loss = 0.06 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53625  Precision @ 1: 0.9750\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4851  Precision @ 1: 0.9702\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9671  Precision @ 1: 0.9671\n",
      "Step 24000: loss = 0.06 (0.011 sec)\n",
      "Step 24100: loss = 0.09 (0.002 sec)\n",
      "Step 24200: loss = 0.04 (0.105 sec)\n",
      "Step 24300: loss = 0.07 (0.002 sec)\n",
      "Step 24400: loss = 0.13 (0.002 sec)\n",
      "Step 24500: loss = 0.14 (0.002 sec)\n",
      "Step 24600: loss = 0.15 (0.002 sec)\n",
      "Step 24700: loss = 0.11 (0.002 sec)\n",
      "Step 24800: loss = 0.04 (0.002 sec)\n",
      "Step 24900: loss = 0.07 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53655  Precision @ 1: 0.9755\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4847  Precision @ 1: 0.9694\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9670  Precision @ 1: 0.9670\n",
      "Step 25000: loss = 0.15 (0.011 sec)\n",
      "Step 25100: loss = 0.06 (0.002 sec)\n",
      "Step 25200: loss = 0.05 (0.002 sec)\n",
      "Step 25300: loss = 0.04 (0.100 sec)\n",
      "Step 25400: loss = 0.09 (0.002 sec)\n",
      "Step 25500: loss = 0.13 (0.002 sec)\n",
      "Step 25600: loss = 0.05 (0.002 sec)\n",
      "Step 25700: loss = 0.05 (0.002 sec)\n",
      "Step 25800: loss = 0.03 (0.002 sec)\n",
      "Step 25900: loss = 0.07 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53755  Precision @ 1: 0.9774\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4853  Precision @ 1: 0.9706\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9679  Precision @ 1: 0.9679\n",
      "Step 26000: loss = 0.14 (0.010 sec)\n",
      "Step 26100: loss = 0.10 (0.002 sec)\n",
      "Step 26200: loss = 0.09 (0.002 sec)\n",
      "Step 26300: loss = 0.06 (0.002 sec)\n",
      "Step 26400: loss = 0.09 (0.106 sec)\n",
      "Step 26500: loss = 0.06 (0.002 sec)\n",
      "Step 26600: loss = 0.05 (0.002 sec)\n",
      "Step 26700: loss = 0.11 (0.002 sec)\n",
      "Step 26800: loss = 0.09 (0.002 sec)\n",
      "Step 26900: loss = 0.13 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53775  Precision @ 1: 0.9777\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4854  Precision @ 1: 0.9708\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9683  Precision @ 1: 0.9683\n",
      "Step 27000: loss = 0.03 (0.015 sec)\n",
      "Step 27100: loss = 0.08 (0.003 sec)\n",
      "Step 27200: loss = 0.05 (0.002 sec)\n",
      "Step 27300: loss = 0.04 (0.002 sec)\n",
      "Step 27400: loss = 0.10 (0.002 sec)\n",
      "Step 27500: loss = 0.08 (0.105 sec)\n",
      "Step 27600: loss = 0.07 (0.002 sec)\n",
      "Step 27700: loss = 0.04 (0.002 sec)\n",
      "Step 27800: loss = 0.07 (0.002 sec)\n",
      "Step 27900: loss = 0.09 (0.002 sec)\n",
      "Training Data Eval:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 55000  Num correct: 53818  Precision @ 1: 0.9785\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4860  Precision @ 1: 0.9720\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9686  Precision @ 1: 0.9686\n",
      "Step 28000: loss = 0.04 (0.010 sec)\n",
      "Step 28100: loss = 0.11 (0.002 sec)\n",
      "Step 28200: loss = 0.08 (0.002 sec)\n",
      "Step 28300: loss = 0.07 (0.002 sec)\n",
      "Step 28400: loss = 0.04 (0.002 sec)\n",
      "Step 28500: loss = 0.05 (0.002 sec)\n",
      "Step 28600: loss = 0.09 (0.103 sec)\n",
      "Step 28700: loss = 0.10 (0.002 sec)\n",
      "Step 28800: loss = 0.15 (0.002 sec)\n",
      "Step 28900: loss = 0.11 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53858  Precision @ 1: 0.9792\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4861  Precision @ 1: 0.9722\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9700  Precision @ 1: 0.9700\n",
      "Step 29000: loss = 0.09 (0.009 sec)\n",
      "Step 29100: loss = 0.19 (0.002 sec)\n",
      "Step 29200: loss = 0.05 (0.002 sec)\n",
      "Step 29300: loss = 0.12 (0.002 sec)\n",
      "Step 29400: loss = 0.11 (0.002 sec)\n",
      "Step 29500: loss = 0.06 (0.002 sec)\n",
      "Step 29600: loss = 0.03 (0.002 sec)\n",
      "Step 29700: loss = 0.04 (0.141 sec)\n",
      "Step 29800: loss = 0.16 (0.002 sec)\n",
      "Step 29900: loss = 0.14 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53936  Precision @ 1: 0.9807\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4860  Precision @ 1: 0.9720\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9703  Precision @ 1: 0.9703\n",
      "Step 30000: loss = 0.04 (0.010 sec)\n",
      "Step 30100: loss = 0.07 (0.002 sec)\n",
      "Step 30200: loss = 0.07 (0.002 sec)\n",
      "Step 30300: loss = 0.10 (0.002 sec)\n",
      "Step 30400: loss = 0.08 (0.002 sec)\n",
      "Step 30500: loss = 0.03 (0.002 sec)\n",
      "Step 30600: loss = 0.05 (0.002 sec)\n",
      "Step 30700: loss = 0.03 (0.002 sec)\n",
      "Step 30800: loss = 0.05 (0.104 sec)\n",
      "Step 30900: loss = 0.11 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53922  Precision @ 1: 0.9804\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4867  Precision @ 1: 0.9734\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9709  Precision @ 1: 0.9709\n",
      "Step 31000: loss = 0.04 (0.014 sec)\n",
      "Step 31100: loss = 0.04 (0.002 sec)\n",
      "Step 31200: loss = 0.10 (0.002 sec)\n",
      "Step 31300: loss = 0.12 (0.002 sec)\n",
      "Step 31400: loss = 0.03 (0.002 sec)\n",
      "Step 31500: loss = 0.06 (0.002 sec)\n",
      "Step 31600: loss = 0.10 (0.002 sec)\n",
      "Step 31700: loss = 0.07 (0.002 sec)\n",
      "Step 31800: loss = 0.08 (0.002 sec)\n",
      "Step 31900: loss = 0.09 (0.106 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 53984  Precision @ 1: 0.9815\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4865  Precision @ 1: 0.9730\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9721  Precision @ 1: 0.9721\n",
      "Step 32000: loss = 0.12 (0.010 sec)\n",
      "Step 32100: loss = 0.10 (0.002 sec)\n",
      "Step 32200: loss = 0.03 (0.002 sec)\n",
      "Step 32300: loss = 0.05 (0.002 sec)\n",
      "Step 32400: loss = 0.04 (0.002 sec)\n",
      "Step 32500: loss = 0.06 (0.002 sec)\n",
      "Step 32600: loss = 0.04 (0.002 sec)\n",
      "Step 32700: loss = 0.10 (0.002 sec)\n",
      "Step 32800: loss = 0.04 (0.002 sec)\n",
      "Step 32900: loss = 0.04 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54053  Precision @ 1: 0.9828\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4869  Precision @ 1: 0.9738\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9730  Precision @ 1: 0.9730\n",
      "Step 33000: loss = 0.13 (0.109 sec)\n",
      "Step 33100: loss = 0.02 (0.002 sec)\n",
      "Step 33200: loss = 0.06 (0.002 sec)\n",
      "Step 33300: loss = 0.10 (0.002 sec)\n",
      "Step 33400: loss = 0.14 (0.002 sec)\n",
      "Step 33500: loss = 0.09 (0.002 sec)\n",
      "Step 33600: loss = 0.04 (0.002 sec)\n",
      "Step 33700: loss = 0.04 (0.002 sec)\n",
      "Step 33800: loss = 0.05 (0.002 sec)\n",
      "Step 33900: loss = 0.05 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54061  Precision @ 1: 0.9829\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4866  Precision @ 1: 0.9732\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9728  Precision @ 1: 0.9728\n",
      "Step 34000: loss = 0.05 (0.010 sec)\n",
      "Step 34100: loss = 0.02 (0.099 sec)\n",
      "Step 34200: loss = 0.14 (0.002 sec)\n",
      "Step 34300: loss = 0.08 (0.002 sec)\n",
      "Step 34400: loss = 0.08 (0.002 sec)\n",
      "Step 34500: loss = 0.02 (0.002 sec)\n",
      "Step 34600: loss = 0.07 (0.002 sec)\n",
      "Step 34700: loss = 0.03 (0.002 sec)\n",
      "Step 34800: loss = 0.03 (0.002 sec)\n",
      "Step 34900: loss = 0.02 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54074  Precision @ 1: 0.9832\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4868  Precision @ 1: 0.9736\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9722  Precision @ 1: 0.9722\n",
      "Step 35000: loss = 0.05 (0.010 sec)\n",
      "Step 35100: loss = 0.08 (0.002 sec)\n",
      "Step 35200: loss = 0.04 (0.107 sec)\n",
      "Step 35300: loss = 0.05 (0.002 sec)\n",
      "Step 35400: loss = 0.05 (0.002 sec)\n",
      "Step 35500: loss = 0.12 (0.002 sec)\n",
      "Step 35600: loss = 0.12 (0.002 sec)\n",
      "Step 35700: loss = 0.04 (0.002 sec)\n",
      "Step 35800: loss = 0.05 (0.002 sec)\n",
      "Step 35900: loss = 0.04 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54162  Precision @ 1: 0.9848\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4873  Precision @ 1: 0.9746\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9737  Precision @ 1: 0.9737\n",
      "Step 36000: loss = 0.04 (0.011 sec)\n",
      "Step 36100: loss = 0.08 (0.002 sec)\n",
      "Step 36200: loss = 0.11 (0.002 sec)\n",
      "Step 36300: loss = 0.05 (0.098 sec)\n",
      "Step 36400: loss = 0.04 (0.002 sec)\n",
      "Step 36500: loss = 0.08 (0.002 sec)\n",
      "Step 36600: loss = 0.01 (0.002 sec)\n",
      "Step 36700: loss = 0.03 (0.002 sec)\n",
      "Step 36800: loss = 0.06 (0.002 sec)\n",
      "Step 36900: loss = 0.01 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54117  Precision @ 1: 0.9839\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4865  Precision @ 1: 0.9730\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9728  Precision @ 1: 0.9728\n",
      "Step 37000: loss = 0.07 (0.012 sec)\n",
      "Step 37100: loss = 0.06 (0.002 sec)\n",
      "Step 37200: loss = 0.06 (0.002 sec)\n",
      "Step 37300: loss = 0.05 (0.002 sec)\n",
      "Step 37400: loss = 0.02 (0.105 sec)\n",
      "Step 37500: loss = 0.14 (0.002 sec)\n",
      "Step 37600: loss = 0.10 (0.002 sec)\n",
      "Step 37700: loss = 0.04 (0.002 sec)\n",
      "Step 37800: loss = 0.03 (0.002 sec)\n",
      "Step 37900: loss = 0.04 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54230  Precision @ 1: 0.9860\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4871  Precision @ 1: 0.9742\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9736  Precision @ 1: 0.9736\n",
      "Step 38000: loss = 0.07 (0.011 sec)\n",
      "Step 38100: loss = 0.04 (0.002 sec)\n",
      "Step 38200: loss = 0.10 (0.003 sec)\n",
      "Step 38300: loss = 0.05 (0.002 sec)\n",
      "Step 38400: loss = 0.09 (0.002 sec)\n",
      "Step 38500: loss = 0.02 (0.105 sec)\n",
      "Step 38600: loss = 0.05 (0.002 sec)\n",
      "Step 38700: loss = 0.02 (0.002 sec)\n",
      "Step 38800: loss = 0.05 (0.002 sec)\n",
      "Step 38900: loss = 0.03 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54248  Precision @ 1: 0.9863\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4870  Precision @ 1: 0.9740\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9747  Precision @ 1: 0.9747\n",
      "Step 39000: loss = 0.17 (0.011 sec)\n",
      "Step 39100: loss = 0.06 (0.002 sec)\n",
      "Step 39200: loss = 0.06 (0.002 sec)\n",
      "Step 39300: loss = 0.02 (0.002 sec)\n",
      "Step 39400: loss = 0.04 (0.002 sec)\n",
      "Step 39500: loss = 0.04 (0.002 sec)\n",
      "Step 39600: loss = 0.08 (0.105 sec)\n",
      "Step 39700: loss = 0.02 (0.002 sec)\n",
      "Step 39800: loss = 0.08 (0.002 sec)\n",
      "Step 39900: loss = 0.07 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54304  Precision @ 1: 0.9873\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4868  Precision @ 1: 0.9736\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9744  Precision @ 1: 0.9744\n",
      "Step 40000: loss = 0.04 (0.010 sec)\n",
      "Step 40100: loss = 0.03 (0.002 sec)\n",
      "Step 40200: loss = 0.10 (0.002 sec)\n",
      "Step 40300: loss = 0.06 (0.002 sec)\n",
      "Step 40400: loss = 0.07 (0.002 sec)\n",
      "Step 40500: loss = 0.06 (0.002 sec)\n",
      "Step 40600: loss = 0.04 (0.002 sec)\n",
      "Step 40700: loss = 0.02 (0.106 sec)\n",
      "Step 40800: loss = 0.02 (0.002 sec)\n",
      "Step 40900: loss = 0.03 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54306  Precision @ 1: 0.9874\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4877  Precision @ 1: 0.9754\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9747  Precision @ 1: 0.9747\n",
      "Step 41000: loss = 0.04 (0.012 sec)\n",
      "Step 41100: loss = 0.04 (0.002 sec)\n",
      "Step 41200: loss = 0.06 (0.002 sec)\n",
      "Step 41300: loss = 0.06 (0.002 sec)\n",
      "Step 41400: loss = 0.04 (0.002 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41500: loss = 0.02 (0.002 sec)\n",
      "Step 41600: loss = 0.05 (0.002 sec)\n",
      "Step 41700: loss = 0.06 (0.002 sec)\n",
      "Step 41800: loss = 0.02 (0.106 sec)\n",
      "Step 41900: loss = 0.04 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54325  Precision @ 1: 0.9877\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4876  Precision @ 1: 0.9752\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9762  Precision @ 1: 0.9762\n",
      "Step 42000: loss = 0.01 (0.011 sec)\n",
      "Step 42100: loss = 0.02 (0.004 sec)\n",
      "Step 42200: loss = 0.03 (0.003 sec)\n",
      "Step 42300: loss = 0.05 (0.003 sec)\n",
      "Step 42400: loss = 0.05 (0.003 sec)\n",
      "Step 42500: loss = 0.03 (0.002 sec)\n",
      "Step 42600: loss = 0.11 (0.002 sec)\n",
      "Step 42700: loss = 0.02 (0.002 sec)\n",
      "Step 42800: loss = 0.04 (0.003 sec)\n",
      "Step 42900: loss = 0.03 (0.127 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54365  Precision @ 1: 0.9885\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4880  Precision @ 1: 0.9760\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9743  Precision @ 1: 0.9743\n",
      "Step 43000: loss = 0.02 (0.015 sec)\n",
      "Step 43100: loss = 0.05 (0.008 sec)\n",
      "Step 43200: loss = 0.04 (0.002 sec)\n",
      "Step 43300: loss = 0.07 (0.002 sec)\n",
      "Step 43400: loss = 0.04 (0.002 sec)\n",
      "Step 43500: loss = 0.06 (0.002 sec)\n",
      "Step 43600: loss = 0.03 (0.002 sec)\n",
      "Step 43700: loss = 0.04 (0.002 sec)\n",
      "Step 43800: loss = 0.06 (0.002 sec)\n",
      "Step 43900: loss = 0.03 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54383  Precision @ 1: 0.9888\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4882  Precision @ 1: 0.9764\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9748  Precision @ 1: 0.9748\n",
      "Step 44000: loss = 0.03 (0.110 sec)\n",
      "Step 44100: loss = 0.03 (0.002 sec)\n",
      "Step 44200: loss = 0.12 (0.002 sec)\n",
      "Step 44300: loss = 0.08 (0.002 sec)\n",
      "Step 44400: loss = 0.06 (0.002 sec)\n",
      "Step 44500: loss = 0.03 (0.002 sec)\n",
      "Step 44600: loss = 0.11 (0.002 sec)\n",
      "Step 44700: loss = 0.01 (0.002 sec)\n",
      "Step 44800: loss = 0.05 (0.002 sec)\n",
      "Step 44900: loss = 0.06 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54413  Precision @ 1: 0.9893\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4886  Precision @ 1: 0.9772\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9758  Precision @ 1: 0.9758\n",
      "Step 45000: loss = 0.03 (0.011 sec)\n",
      "Step 45100: loss = 0.02 (0.098 sec)\n",
      "Step 45200: loss = 0.10 (0.002 sec)\n",
      "Step 45300: loss = 0.03 (0.002 sec)\n",
      "Step 45400: loss = 0.02 (0.002 sec)\n",
      "Step 45500: loss = 0.03 (0.002 sec)\n",
      "Step 45600: loss = 0.03 (0.006 sec)\n",
      "Step 45700: loss = 0.06 (0.003 sec)\n",
      "Step 45800: loss = 0.06 (0.003 sec)\n",
      "Step 45900: loss = 0.03 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54402  Precision @ 1: 0.9891\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4884  Precision @ 1: 0.9768\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9756  Precision @ 1: 0.9756\n",
      "Step 46000: loss = 0.02 (0.010 sec)\n",
      "Step 46100: loss = 0.04 (0.002 sec)\n",
      "Step 46200: loss = 0.07 (0.108 sec)\n",
      "Step 46300: loss = 0.04 (0.002 sec)\n",
      "Step 46400: loss = 0.04 (0.002 sec)\n",
      "Step 46500: loss = 0.04 (0.002 sec)\n",
      "Step 46600: loss = 0.04 (0.002 sec)\n",
      "Step 46700: loss = 0.04 (0.002 sec)\n",
      "Step 46800: loss = 0.03 (0.003 sec)\n",
      "Step 46900: loss = 0.05 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54405  Precision @ 1: 0.9892\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4886  Precision @ 1: 0.9772\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9747  Precision @ 1: 0.9747\n",
      "Step 47000: loss = 0.02 (0.010 sec)\n",
      "Step 47100: loss = 0.04 (0.002 sec)\n",
      "Step 47200: loss = 0.04 (0.002 sec)\n",
      "Step 47300: loss = 0.02 (0.099 sec)\n",
      "Step 47400: loss = 0.02 (0.002 sec)\n",
      "Step 47500: loss = 0.05 (0.002 sec)\n",
      "Step 47600: loss = 0.01 (0.002 sec)\n",
      "Step 47700: loss = 0.11 (0.002 sec)\n",
      "Step 47800: loss = 0.10 (0.002 sec)\n",
      "Step 47900: loss = 0.03 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54471  Precision @ 1: 0.9904\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4885  Precision @ 1: 0.9770\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9758  Precision @ 1: 0.9758\n",
      "Step 48000: loss = 0.03 (0.010 sec)\n",
      "Step 48100: loss = 0.05 (0.002 sec)\n",
      "Step 48200: loss = 0.02 (0.002 sec)\n",
      "Step 48300: loss = 0.02 (0.002 sec)\n",
      "Step 48400: loss = 0.08 (0.104 sec)\n",
      "Step 48500: loss = 0.02 (0.003 sec)\n",
      "Step 48600: loss = 0.06 (0.002 sec)\n",
      "Step 48700: loss = 0.02 (0.002 sec)\n",
      "Step 48800: loss = 0.03 (0.002 sec)\n",
      "Step 48900: loss = 0.04 (0.003 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54489  Precision @ 1: 0.9907\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4883  Precision @ 1: 0.9766\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9762  Precision @ 1: 0.9762\n",
      "Step 49000: loss = 0.05 (0.011 sec)\n",
      "Step 49100: loss = 0.02 (0.002 sec)\n",
      "Step 49200: loss = 0.05 (0.002 sec)\n",
      "Step 49300: loss = 0.04 (0.002 sec)\n",
      "Step 49400: loss = 0.03 (0.002 sec)\n",
      "Step 49500: loss = 0.02 (0.110 sec)\n",
      "Step 49600: loss = 0.05 (0.002 sec)\n",
      "Step 49700: loss = 0.03 (0.002 sec)\n",
      "Step 49800: loss = 0.02 (0.002 sec)\n",
      "Step 49900: loss = 0.05 (0.002 sec)\n",
      "Training Data Eval:\n",
      "Num examples: 55000  Num correct: 54519  Precision @ 1: 0.9913\n",
      "Validation Data Eval:\n",
      "Num examples: 5000  Num correct: 4883  Precision @ 1: 0.9766\n",
      "Test Data Eval:\n",
      "Num examples: 10000  Num correct: 9765  Precision @ 1: 0.9765\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "\n",
    "    feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "\n",
    "    _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "\n",
    "    if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "        checkpoint_file = os.path.join(log_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        # Evaluate against the training set.\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_sets.train)\n",
    "        # Evaluate against the validation set.\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_sets.validation)\n",
    "        # Evaluate against the test set.\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_sets.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "steps_per_epoch = data_sets.test.num_examples // batch_size\n",
    "for step in range(steps_per_epoch):\n",
    "    images_feed, labels_feed = data_sets.test.next_batch(batch_size)\n",
    "    feed_dict = {images_placeholder: images_feed, labels_placeholder: labels_feed}\n",
    "    prediction_value = sess.run(prediction, feed_dict=feed_dict)\n",
    "    incorrect_index = np.where(prediction_value != labels_feed)\n",
    "    # print(incorrect_index)\n",
    "    for i in incorrect_index[0]:\n",
    "        pred = prediction_value[i]\n",
    "        label = labels_feed[i]\n",
    "        # print(pred, label)\n",
    "        image = images_feed[i]\n",
    "        image = np.multiply(image, 255.0)\n",
    "        image = image.astype(np.uint8)\n",
    "        image = image.reshape(28, 28)\n",
    "        im = Image.fromarray(image)\n",
    "        if not os.path.exists(os.path.join(input_data_dir, 'incorrect', str(label))):\n",
    "            os.makedirs(os.path.join(input_data_dir, 'incorrect', str(label)))\n",
    "        filename = os.path.join(input_data_dir, 'incorrect', str(label),\n",
    "                                '%d_%d_%d.bmp' % (pred, np.mean(image) * 1e6, np.std(image) * 1e6))\n",
    "        im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
